{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Enhanced Streaming Language Modeling Data Pipeline with Hugging Face Datasets\n",
      "----------------------------------------------------------------------------\n",
      "Goal:\n",
      " Demonstrate an enhanced streaming LM pipeline that:\n",
      " - Uses a different dataset (WikiText-103) for larger, more diverse text content\n",
      " - Employs DistilGPT-2 tokenizer for faster processing\n",
      " - Processes data without loading the entire dataset into RAM\n",
      " - Tokenizes on the fly with custom preprocessing\n",
      " - Concatenates text and chunks into larger fixed-length blocks (256 tokens)\n",
      " - Produces batches ready for training in PyTorch\n",
      " - Includes data statistics and memory usage tracking\n",
      "\n",
      "Key Enhancements:\n",
      " 1. Different dataset: WikiText-103 (larger) instead of WikiText-2\n",
      " 2. Different model: DistilGPT-2 instead of GPT-2\n",
      " 3. Larger block size: 256 tokens for better context\n",
      " 4. Enhanced monitoring: Statistics and memory tracking\n",
      " 5. Additional analysis: Token distribution visualization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Streaming Language Modeling Data Pipeline with Hugging Face Datasets\n",
    "----------------------------------------------------------------------------\n",
    "Goal:\n",
    " Demonstrate an enhanced streaming LM pipeline that:\n",
    " - Uses a different dataset (WikiText-103) for larger, more diverse text content\n",
    " - Employs DistilGPT-2 tokenizer for faster processing\n",
    " - Processes data without loading the entire dataset into RAM\n",
    " - Tokenizes on the fly with custom preprocessing\n",
    " - Concatenates text and chunks into larger fixed-length blocks (256 tokens)\n",
    " - Produces batches ready for training in PyTorch\n",
    " - Includes data statistics and memory usage tracking\n",
    "\n",
    "Key Enhancements:\n",
    " 1. Different dataset: WikiText-103 (larger) instead of WikiText-2\n",
    " 2. Different model: DistilGPT-2 instead of GPT-2\n",
    " 3. Larger block size: 256 tokens for better context\n",
    " 4. Enhanced monitoring: Statistics and memory tracking\n",
    " 5. Additional analysis: Token distribution visualization\n",
    "\"\"\"\n",
    "print(__doc__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Load the dataset in STREAMING mode\n",
    "# ============================================================\n",
    "# Using WikiText-103 (larger version) instead of WikiText-2\n",
    "# This provides more diverse content while maintaining compatibility\n",
    "# Streaming mode returns an IterableDataset — you can iterate over it\n",
    "# without having all the data in memory at once.\n",
    "stream_dataset = load_dataset(\n",
    "    \"wikitext\", \n",
    "    \"wikitext-103-raw-v1\", \n",
    "    split=\"train\", \n",
    "    streaming=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Analyzing dataset characteristics...\n",
      "\n",
      "Sample texts (first 200 chars each):\n",
      "\n",
      "Example 1:  = Valkyria Chronicles III = \n",
      "...\n",
      "\n",
      "Example 2:  Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ p...\n",
      "\n",
      "Example 3:  The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adju...\n",
      "\n",
      "Example 4:  It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year...\n",
      "\n",
      "Example 5:  = = Gameplay = = \n",
      "...\n",
      "\n",
      "Average text length (first 5 samples): 371 characters\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1.5. Sample and analyze dataset characteristics\n",
    "# ============================================================\n",
    "print(\"Analyzing dataset characteristics...\")\n",
    "sample_count = 0\n",
    "total_chars = 0\n",
    "sample_texts = []\n",
    "\n",
    "for example in stream_dataset:\n",
    "    if sample_count < 5:  # Sample first 5 examples\n",
    "        # C4 dataset has 'text' field\n",
    "        text = example.get(\"text\", \"\")\n",
    "        if text:  # Only process non-empty texts\n",
    "            sample_texts.append(text[:200])  # First 200 chars\n",
    "            total_chars += len(text)\n",
    "            sample_count += 1\n",
    "    if sample_count >= 5:\n",
    "        break\n",
    "\n",
    "print(f\"\\nSample texts (first 200 chars each):\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"\\nExample {i+1}: {text}...\")\n",
    "\n",
    "if sample_count > 0:\n",
    "    print(f\"\\nAverage text length (first {sample_count} samples): {total_chars / sample_count:.0f} characters\")\n",
    "else:\n",
    "    print(\"\\nNo samples found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memory usage after tokenizer initialization: 459.79 MB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2.5. Memory usage tracking (after tokenizer initialization)\n",
    "# ============================================================\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "# Track memory after tokenizer is loaded\n",
    "initial_memory = get_memory_usage()\n",
    "print(f\"Memory usage after tokenizer initialization: {initial_memory:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. Initialize the tokenizer\n",
    "# ============================================================\n",
    "# Using DistilGPT-2 - a distilled version of GPT-2\n",
    "# Faster and smaller while maintaining good performance\n",
    "# For DistilGPT-2, there is no pad token by default, so we set pad_token = eos_token.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. Tokenization step\n",
    "# ============================================================\n",
    "# We do NOT pad/truncate here — we want raw token sequences.\n",
    "# This keeps flexibility to later concatenate across documents.\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "# Map tokenization lazily over the streaming dataset\n",
    "tokenized_stream = stream_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. Rolling buffer for grouping into fixed-length blocks\n",
    "# ============================================================\n",
    "# Using larger block size (256) for better context understanding\n",
    "# This allows models to see longer sequences\n",
    "# Because streaming datasets are iterators, we can't look ahead arbitrarily.\n",
    "# We'll keep a buffer that stores leftover tokens from the previous batch,\n",
    "# so we can concatenate and chunk consistently.\n",
    "block_size = 256  # Changed from 128 to 256 for better context\n",
    "\n",
    "def group_texts_streaming(dataset_iter, block_size):\n",
    "    buffer = []\n",
    "    for example in dataset_iter:\n",
    "        buffer.extend(example[\"input_ids\"])\n",
    "        while len(buffer) >= block_size:\n",
    "            chunk = buffer[:block_size]\n",
    "            buffer = buffer[block_size:]\n",
    "            yield {\n",
    "                \"input_ids\": chunk,\n",
    "                \"attention_mask\": [1] * block_size\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. Wrap generator in an IterableDataset\n",
    "# ============================================================\n",
    "class StreamingLMIterableDataset(IterableDataset):\n",
    "    def __init__(self, hf_iterable_dataset, block_size):\n",
    "        self.dataset = hf_iterable_dataset\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return group_texts_streaming(self.dataset, self.block_size)\n",
    "\n",
    "grouped_iterable_dataset = StreamingLMIterableDataset(tokenized_stream, block_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Collate function for batches\n",
    "# ============================================================\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([ex[\"input_ids\"] for ex in batch], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([ex[\"attention_mask\"] for ex in batch], dtype=torch.long)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": input_ids.clone()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. DataLoader for streaming data\n",
    "# ============================================================\n",
    "# Using larger batch size for more efficient training\n",
    "train_loader = DataLoader(grouped_iterable_dataset, batch_size=16, collate_fn=collate_fn)  # Changed from 8 to 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample streaming batches:\n",
      "Testing with 5 batches (batch_size=16, block_size=256)...\n",
      "Batch 0 -> input_ids shape: torch.Size([16, 256])\n",
      "         -> Total tokens: 4096\n",
      "         -> attention_mask shape: torch.Size([16, 256])\n",
      "         -> labels shape: torch.Size([16, 256])\n",
      "Batch 1 -> input_ids shape: torch.Size([16, 256])\n",
      "         -> Total tokens: 4096\n",
      "         -> attention_mask shape: torch.Size([16, 256])\n",
      "         -> labels shape: torch.Size([16, 256])\n",
      "Batch 2 -> input_ids shape: torch.Size([16, 256])\n",
      "         -> Total tokens: 4096\n",
      "         -> attention_mask shape: torch.Size([16, 256])\n",
      "         -> labels shape: torch.Size([16, 256])\n",
      "Batch 3 -> input_ids shape: torch.Size([16, 256])\n",
      "         -> Total tokens: 4096\n",
      "         -> attention_mask shape: torch.Size([16, 256])\n",
      "         -> labels shape: torch.Size([16, 256])\n",
      "Batch 4 -> input_ids shape: torch.Size([16, 256])\n",
      "         -> Total tokens: 4096\n",
      "         -> attention_mask shape: torch.Size([16, 256])\n",
      "         -> labels shape: torch.Size([16, 256])\n",
      "\n",
      "============================================================\n",
      "Batch Statistics:\n",
      "  Average batch size: 16.0\n",
      "  Total tokens processed: 20,480\n",
      "  Memory usage after processing: 537.17 MB\n",
      "  Memory increase: 77.39 MB\n",
      "============================================================\n",
      "\n",
      "✅ Test completed successfully! All batches have correct shapes.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. Iterate over batches with enhanced monitoring\n",
    "# ============================================================\n",
    "print(\"Sample streaming batches:\")\n",
    "print(\"Testing with 5 batches (batch_size=16, block_size=256)...\")\n",
    "\n",
    "batch_shapes = []\n",
    "token_counts = []\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    shape = batch['input_ids'].shape\n",
    "    batch_shapes.append(shape)\n",
    "    token_counts.append(batch['input_ids'].numel())\n",
    "    \n",
    "    print(f\"Batch {i} -> input_ids shape: {shape}\")\n",
    "    print(f\"         -> Total tokens: {batch['input_ids'].numel()}\")\n",
    "    print(f\"         -> attention_mask shape: {batch['attention_mask'].shape}\")\n",
    "    print(f\"         -> labels shape: {batch['labels'].shape}\")\n",
    "    \n",
    "    # Validation checks\n",
    "    assert batch['input_ids'].shape == batch['labels'].shape, \"Input IDs and labels must have same shape\"\n",
    "    assert batch['input_ids'].shape[1] == block_size, f\"Sequence length must be {block_size}\"\n",
    "    \n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Batch Statistics:\")\n",
    "print(f\"  Average batch size: {sum(s[0] for s in batch_shapes) / len(batch_shapes)}\")\n",
    "print(f\"  Total tokens processed: {sum(token_counts):,}\")\n",
    "print(f\"  Memory usage after processing: {get_memory_usage():.2f} MB\")\n",
    "print(f\"  Memory increase: {get_memory_usage() - initial_memory:.2f} MB\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n✅ Test completed successfully! All batches have correct shapes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "Enhanced Validation Tests\n",
      "============================================================\n",
      "\n",
      "Tokenizer Information:\n",
      "  Model: DistilGPT-2\n",
      "  Vocab size: 50,257\n",
      "  Block size: 256\n",
      "  Batch size: 16\n",
      "\n",
      "Sample Batch Data Types:\n",
      "  input_ids dtype: torch.int64\n",
      "  attention_mask dtype: torch.int64\n",
      "  labels dtype: torch.int64\n",
      "\n",
      "Sample Batch Value Ranges:\n",
      "  input_ids min: 11, max: 49907\n",
      "  All values are valid token IDs: True\n",
      "  Values within vocab range: True\n",
      "\n",
      "Top 10 Most Frequent Token IDs in Sample Batch:\n",
      "  Token ID   262 ( the                ):  207 occurrences\n",
      "  Token ID   837 ( ,                  ):  178 occurrences\n",
      "  Token ID   764 ( .                  ):  128 occurrences\n",
      "  Token ID   284 ( to                 ):   81 occurrences\n",
      "  Token ID   286 ( of                 ):   75 occurrences\n",
      "  Token ID   290 ( and                ):   73 occurrences\n",
      "  Token ID   257 ( a                  ):   60 occurrences\n",
      "  Token ID   569 ( V                  ):   53 occurrences\n",
      "  Token ID 18354 (alky                ):   53 occurrences\n",
      "  Token ID  7496 (ria                 ):   49 occurrences\n",
      "\n",
      "============================================================\n",
      "✅ All validation tests passed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. Enhanced validation and token analysis\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Enhanced Validation Tests\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTokenizer Information:\")\n",
    "print(f\"  Model: DistilGPT-2\")\n",
    "print(f\"  Vocab size: {len(tokenizer):,}\")\n",
    "print(f\"  Block size: {block_size}\")\n",
    "print(f\"  Batch size: 16\")\n",
    "\n",
    "# Get a sample batch for analysis\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\nSample Batch Data Types:\")\n",
    "print(f\"  input_ids dtype: {sample_batch['input_ids'].dtype}\")\n",
    "print(f\"  attention_mask dtype: {sample_batch['attention_mask'].dtype}\")\n",
    "print(f\"  labels dtype: {sample_batch['labels'].dtype}\")\n",
    "\n",
    "print(f\"\\nSample Batch Value Ranges:\")\n",
    "min_val = sample_batch['input_ids'].min().item()\n",
    "max_val = sample_batch['input_ids'].max().item()\n",
    "print(f\"  input_ids min: {min_val}, max: {max_val}\")\n",
    "print(f\"  All values are valid token IDs: {min_val >= 0}\")\n",
    "print(f\"  Values within vocab range: {max_val < len(tokenizer)}\")\n",
    "\n",
    "# Token frequency analysis\n",
    "all_tokens = sample_batch['input_ids'].flatten().tolist()\n",
    "token_freq = Counter(all_tokens)\n",
    "most_common = token_freq.most_common(10)\n",
    "\n",
    "print(f\"\\nTop 10 Most Frequent Token IDs in Sample Batch:\")\n",
    "for token_id, count in most_common:\n",
    "    try:\n",
    "        token = tokenizer.decode([token_id])\n",
    "        print(f\"  Token ID {token_id:5d} ({token:20s}): {count:4d} occurrences\")\n",
    "    except:\n",
    "        print(f\"  Token ID {token_id:5d}: {count:4d} occurrences\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ All validation tests passed!\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}