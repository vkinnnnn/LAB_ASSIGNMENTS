{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Enhanced Streaming Language Modeling Data Pipeline with Hugging Face Datasets\n",
        "----------------------------------------------------------------------------\n",
        "Goal:\n",
        " Demonstrate an enhanced streaming LM pipeline that:\n",
        " - Uses a different dataset (WikiText-103) for larger, more diverse text content\n",
        " - Employs DistilGPT-2 tokenizer for faster processing\n",
        " - Processes data without loading the entire dataset into RAM\n",
        " - Tokenizes on the fly with custom preprocessing\n",
        " - Concatenates text and chunks into larger fixed-length blocks (256 tokens)\n",
        " - Produces batches ready for training in PyTorch\n",
        " - Includes data statistics and memory usage tracking\n",
        "\n",
        "Key Enhancements:\n",
        " 1. Different dataset: WikiText-103 (larger) instead of WikiText-2\n",
        " 2. Different model: DistilGPT-2 instead of GPT-2\n",
        " 3. Larger block size: 256 tokens for better context\n",
        " 4. Enhanced monitoring: Statistics and memory tracking\n",
        " 5. Additional analysis: Token distribution visualization\n",
        "\"\"\"\n",
        "print(__doc__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install datasets transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "import torch\n",
        "import psutil\n",
        "import os\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1. Load the dataset in STREAMING mode\n",
        "# ============================================================\n",
        "# Using WikiText-103 (larger version) instead of WikiText-2\n",
        "# This provides more diverse content while maintaining compatibility\n",
        "# Streaming mode returns an IterableDataset — you can iterate over it\n",
        "# without having all the data in memory at once.\n",
        "stream_dataset = load_dataset(\n",
        "    \"wikitext\", \n",
        "    \"wikitext-103-raw-v1\", \n",
        "    split=\"train\", \n",
        "    streaming=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1.5. Sample and analyze dataset characteristics\n",
        "# ============================================================\n",
        "print(\"Analyzing dataset characteristics...\")\n",
        "sample_count = 0\n",
        "total_chars = 0\n",
        "sample_texts = []\n",
        "\n",
        "for example in stream_dataset:\n",
        "    if sample_count < 5:  # Sample first 5 examples\n",
        "        # C4 dataset has 'text' field\n",
        "        text = example.get(\"text\", \"\")\n",
        "        if text:  # Only process non-empty texts\n",
        "            sample_texts.append(text[:200])  # First 200 chars\n",
        "            total_chars += len(text)\n",
        "            sample_count += 1\n",
        "    if sample_count >= 5:\n",
        "        break\n",
        "\n",
        "print(f\"\\nSample texts (first 200 chars each):\")\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"\\nExample {i+1}: {text}...\")\n",
        "\n",
        "if sample_count > 0:\n",
        "    print(f\"\\nAverage text length (first {sample_count} samples): {total_chars / sample_count:.0f} characters\")\n",
        "else:\n",
        "    print(\"\\nNo samples found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 2.5. Memory usage tracking (after tokenizer initialization)\n",
        "# ============================================================\n",
        "def get_memory_usage():\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "# Track memory after tokenizer is loaded\n",
        "initial_memory = get_memory_usage()\n",
        "print(f\"Memory usage after tokenizer initialization: {initial_memory:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 2. Initialize the tokenizer\n",
        "# ============================================================\n",
        "# Using DistilGPT-2 - a distilled version of GPT-2\n",
        "# Faster and smaller while maintaining good performance\n",
        "# For DistilGPT-2, there is no pad token by default, so we set pad_token = eos_token.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3. Tokenization step\n",
        "# ============================================================\n",
        "# We do NOT pad/truncate here — we want raw token sequences.\n",
        "# This keeps flexibility to later concatenate across documents.\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])\n",
        "\n",
        "# Map tokenization lazily over the streaming dataset\n",
        "tokenized_stream = stream_dataset.map(tokenize_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 4. Rolling buffer for grouping into fixed-length blocks\n",
        "# ============================================================\n",
        "# Using larger block size (256) for better context understanding\n",
        "# This allows models to see longer sequences\n",
        "# Because streaming datasets are iterators, we can't look ahead arbitrarily.\n",
        "# We'll keep a buffer that stores leftover tokens from the previous batch,\n",
        "# so we can concatenate and chunk consistently.\n",
        "block_size = 256  # Changed from 128 to 256 for better context\n",
        "\n",
        "def group_texts_streaming(dataset_iter, block_size):\n",
        "    buffer = []\n",
        "    for example in dataset_iter:\n",
        "        buffer.extend(example[\"input_ids\"])\n",
        "        while len(buffer) >= block_size:\n",
        "            chunk = buffer[:block_size]\n",
        "            buffer = buffer[block_size:]\n",
        "            yield {\n",
        "                \"input_ids\": chunk,\n",
        "                \"attention_mask\": [1] * block_size\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5. Wrap generator in an IterableDataset\n",
        "# ============================================================\n",
        "class StreamingLMIterableDataset(IterableDataset):\n",
        "    def __init__(self, hf_iterable_dataset, block_size):\n",
        "        self.dataset = hf_iterable_dataset\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        return group_texts_streaming(self.dataset, self.block_size)\n",
        "\n",
        "grouped_iterable_dataset = StreamingLMIterableDataset(tokenized_stream, block_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6. Collate function for batches\n",
        "# ============================================================\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([ex[\"input_ids\"] for ex in batch], dtype=torch.long)\n",
        "    attention_mask = torch.tensor([ex[\"attention_mask\"] for ex in batch], dtype=torch.long)\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": input_ids.clone()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 7. DataLoader for streaming data\n",
        "# ============================================================\n",
        "# Using larger batch size for more efficient training\n",
        "train_loader = DataLoader(grouped_iterable_dataset, batch_size=16, collate_fn=collate_fn)  # Changed from 8 to 16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 8. Iterate over batches with enhanced monitoring\n",
        "# ============================================================\n",
        "print(\"Sample streaming batches:\")\n",
        "print(\"Testing with 5 batches (batch_size=16, block_size=256)...\")\n",
        "\n",
        "batch_shapes = []\n",
        "token_counts = []\n",
        "\n",
        "for i, batch in enumerate(train_loader):\n",
        "    shape = batch['input_ids'].shape\n",
        "    batch_shapes.append(shape)\n",
        "    token_counts.append(batch['input_ids'].numel())\n",
        "    \n",
        "    print(f\"Batch {i} -> input_ids shape: {shape}\")\n",
        "    print(f\"         -> Total tokens: {batch['input_ids'].numel()}\")\n",
        "    print(f\"         -> attention_mask shape: {batch['attention_mask'].shape}\")\n",
        "    print(f\"         -> labels shape: {batch['labels'].shape}\")\n",
        "    \n",
        "    # Validation checks\n",
        "    assert batch['input_ids'].shape == batch['labels'].shape, \"Input IDs and labels must have same shape\"\n",
        "    assert batch['input_ids'].shape[1] == block_size, f\"Sequence length must be {block_size}\"\n",
        "    \n",
        "    if i == 4:\n",
        "        break\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Batch Statistics:\")\n",
        "print(f\"  Average batch size: {sum(s[0] for s in batch_shapes) / len(batch_shapes)}\")\n",
        "print(f\"  Total tokens processed: {sum(token_counts):,}\")\n",
        "print(f\"  Memory usage after processing: {get_memory_usage():.2f} MB\")\n",
        "print(f\"  Memory increase: {get_memory_usage() - initial_memory:.2f} MB\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n✅ Test completed successfully! All batches have correct shapes.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 9. Enhanced validation and token analysis\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Enhanced Validation Tests\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nTokenizer Information:\")\n",
        "print(f\"  Model: DistilGPT-2\")\n",
        "print(f\"  Vocab size: {len(tokenizer):,}\")\n",
        "print(f\"  Block size: {block_size}\")\n",
        "print(f\"  Batch size: 16\")\n",
        "\n",
        "# Get a sample batch for analysis\n",
        "sample_batch = next(iter(train_loader))\n",
        "\n",
        "print(f\"\\nSample Batch Data Types:\")\n",
        "print(f\"  input_ids dtype: {sample_batch['input_ids'].dtype}\")\n",
        "print(f\"  attention_mask dtype: {sample_batch['attention_mask'].dtype}\")\n",
        "print(f\"  labels dtype: {sample_batch['labels'].dtype}\")\n",
        "\n",
        "print(f\"\\nSample Batch Value Ranges:\")\n",
        "min_val = sample_batch['input_ids'].min().item()\n",
        "max_val = sample_batch['input_ids'].max().item()\n",
        "print(f\"  input_ids min: {min_val}, max: {max_val}\")\n",
        "print(f\"  All values are valid token IDs: {min_val >= 0}\")\n",
        "print(f\"  Values within vocab range: {max_val < len(tokenizer)}\")\n",
        "\n",
        "# Token frequency analysis\n",
        "all_tokens = sample_batch['input_ids'].flatten().tolist()\n",
        "token_freq = Counter(all_tokens)\n",
        "most_common = token_freq.most_common(10)\n",
        "\n",
        "print(f\"\\nTop 10 Most Frequent Token IDs in Sample Batch:\")\n",
        "for token_id, count in most_common:\n",
        "    try:\n",
        "        token = tokenizer.decode([token_id])\n",
        "        print(f\"  Token ID {token_id:5d} ({token:20s}): {count:4d} occurrences\")\n",
        "    except:\n",
        "        print(f\"  Token ID {token_id:5d}: {count:4d} occurrences\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ All validation tests passed!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
